{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Object Oriented Code for the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np     #Importing this library to make computaion slightly faster\n",
    "    \n",
    "    ###START OF NEURON CLASS###\n",
    "class Neuron:     #Defining a Neuron Class that can only be called from inside the Layer Class\n",
    "    def __init__(self):  #Initialization block for the Neuron. Doesn't take any inputs\n",
    "        self.type = 'relu' #By default, a neuron object will be of the 'relu' type\n",
    "        self.fp = 0 #To store the value of the forward propagation for the given neuron\n",
    "        self.z = 0 #To store the wrighted sum\n",
    "        self.W = [] #To store the input weights\n",
    "        self.b = 0 #To store the bias of the neuron\n",
    "        self.ac_1 = [] #To store the activations(inputs) from the previous layer\n",
    "        self.dz=0 #To store the gradient generated during backpropagation\n",
    "    def _weighted_sum(self): #Calculating the weighted sum z = w*x + b\n",
    "        summ = 0\n",
    "        for i in range(len(self.ac_1)):\n",
    "            summ+=self.ac_1[i]*self.W[i]\n",
    "        return summ + self.b\n",
    " \n",
    "    def _activation_fn(self): #Calculating the activation based on the type of the neuron\n",
    "        if(self.type=='relu'): # a = max(0,z)\n",
    "            if self.z > 0:\n",
    "                return self.z\n",
    "            return 0\n",
    "        elif(self.type=='sigm'): # a = 1/(1+e^-z)\n",
    "            return 1/(1+np.exp(-self.z))\n",
    "        else:\n",
    "            raise Exception('Invalid Activation Function')\n",
    "\n",
    "    def forward_propagate(self,inp,weights,bias,choice='relu'): #Calculating the output of the neuron given inputs, weights and bias\n",
    "        assert(len(inp)==len(weights))\n",
    "        self.type=choice\n",
    "        self.ac_1 = inp\n",
    "        self.W = weights\n",
    "        self.b = bias\n",
    "        self.z = self._weighted_sum(),\n",
    "        self.fp = self._activation_fn()\n",
    "        return self.fp\n",
    "\n",
    "    def _back_activation_fn(self): #Calculating the derivative wrt to the cost based on the activation function being used\n",
    "        if(self.type=='relu'):\n",
    "             if(self.z<0):\n",
    "                return 0\n",
    "             else:\n",
    "                return self.dac\n",
    "        elif(self.type=='sigm'):\n",
    "            return self.dac*self.fp*(1-self.fp)\n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "\n",
    "    def back_propagate(self,dac): #Outputs the backpropagation outputs to calculate the gradients for the current and the previous layers\n",
    "        self.dac = dac\n",
    "        self.dz = self._back_activation_fn()\n",
    "        m = len(self.ac_1)\n",
    "        dw = np.array(self.ac_1)*self.dz\n",
    "        db = self.dz\n",
    "        dac_1 = np.array(self.W)*self.dz\n",
    "        return dw,db,dac_1\n",
    "###END OF NEURON CLASS###\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START OF LAYER CLASS###\n",
    "class Layer:      #Defining a Layer class that can only be called from inside the Neural_Net Class\n",
    "    def __init__(self, n_neurons): #Initialization block for the Layer Class. Takes the number of Neurons to be present as input.\n",
    "        self.neurons = [] #List to store the Neuron objects created using the Neuron Class\n",
    "        self.n_neurons = n_neurons #Storing the input value of the number of neurons to be present in the Layer\n",
    "        for i in range(self.n_neurons): #Populating the Layer with the objects of the Neuron Class\n",
    "            self.neurons.append(Neuron())\n",
    "    def forward_propagate(self, inputs, weights,bias,choice='relu'):#Outputs the forward propagation of the layers to be used as input for furthe layers\n",
    "        output = []\n",
    "        for i in range(self.n_neurons):\n",
    "            output.append(self.neurons[i].forward_propagate(inputs,weights[i],bias[i],choice))\n",
    "        return output\n",
    "\n",
    "    def back_propagate(self,dacs): #Outputs the gradients required to update weights and biases of the current layer \n",
    "        dacs=np.array(dacs.mean(axis=0))\n",
    "        dws = []\n",
    "        dbs = []\n",
    "        dac_1s = []\n",
    "        for i in range(dacs.shape[0]):\n",
    "            temp1,temp2,temp3 = self.neurons[i].back_propagate(dacs[i])\n",
    "            dws.append(temp1)\n",
    "            dbs.append(temp2)\n",
    "            dac_1s.append(temp3)\n",
    "        return np.array(dws),np.array(dbs),np.array(dac_1s)\n",
    "###END OF LAYER CLASS###\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START OF NEURAL_NET CLASS###\n",
    "class Neural_Net:\n",
    "    def __init__(self, layers): #Initialization block for the Neural Network. Takes the number of layers and the the number of neurons per layer as input in the form of a list\n",
    "        self.n_layers=len(layers)   #Keeps track of the number of hidden layers in the Neural Network\n",
    "        self.inputs=None  #To store the inputs during each iteration of forward propagation\n",
    "        self.outputs=None #To store the corresponding correct outputs during each iteration of forward propagation\n",
    "        self.Layers=[] #List to store all the objects created using the Layer Class\n",
    "        self.Weights=[] #List to store all the Weights of the Network\n",
    "        self.WGrads = [] #List to store all the Gradients of the Weights generated after every epoch\n",
    "        self.Bias=[] #List to store all the Biases of the Network\n",
    "        self.BGrads = [] #List to store the Gradients of the Biases generated after every epoch\n",
    "        self.layers=layers #Storing the input list used to create the Network Architecture,\n",
    "        self.learning_rate = 0 #Learning Rate for the network that is used for Updating the Weights\n",
    "        self.n_inps = 0 #Storing the number of inputs to the network\n",
    "        self.inputs_val = None #To store the inputs of the validation/test set for each iteration fo the forward propagation\n",
    "        self.outputs_val = None #To store the corresponding correct outputs of the validation/test set\n",
    "\n",
    "    def _showWandB(self): #Outputs the weights and biases of the network. Used for sanity checks\n",
    "        print('Weights:',self.Weights)\n",
    "        print('Bias:',self.Bias)\n",
    "\n",
    "    def _showGWandBG(self): #Outputs the gradients of the weights and biases of the network. Used for sanity checks\n",
    "        print('WGrads:',self.WGrads)\n",
    "        print('BGrads:',self.BGrads)\n",
    "\n",
    "    def _initialize(self): #Initializing the Network with the appropriate parameters\n",
    "        for i in range(self.n_layers):\n",
    "            self.Layers.append(Layer(self.layers[i]))\n",
    "            if i==0:\n",
    "                self.Weights.append(np.random.randn(self.layers[i],len(self.inputs[0])))\n",
    "                self.Bias.append(np.random.randn(self.layers[0]))\n",
    "            else:\n",
    "                self.Weights.append(np.random.randn(self.layers[i],self.layers[i-1]))\n",
    "                self.Bias.append(np.random.randn(self.layers[i]))\n",
    "        self.Layers.append(Layer(1))\n",
    "        self.Weights.append(np.random.randn(1,self.layers[self.n_layers-1]))\n",
    "        self.Bias.append(np.random.randn(1))\n",
    "\n",
    "    def _forward_propagate(self,temp): #Outputs the predictive value of the network wrt to the input sample and current weights nad biases\n",
    "        for i in range(len(self.Layers)):\n",
    "            if i==0:\n",
    "                temp = self.Layers[i].forward_propagate(temp,self.Weights[i],self.Bias[i],'relu')\n",
    "            elif i==self.n_layers:\n",
    "                temp = self.Layers[i].forward_propagate(temp,self.Weights[i],self.Bias[i],choice='sigm')\n",
    "            else:\n",
    "                temp = self.Layers[i].forward_propagate(temp,self.Weights[i],self.Bias[i],'relu')\n",
    "        return temp\n",
    "\n",
    "    def _compute_cost(self,pred,act,cost_type = 'crs_ent'): #Computes the cost using the correct outputs\n",
    "        temp_out = np.array(pred)\n",
    "        act=np.array(act)\n",
    "        act = np.expand_dims(act,axis=0)\n",
    "        m = act.shape[0]\n",
    "        if(cost_type == 'mse'):\n",
    "            cost = (1/(2*m))*(np.array(temp_out)-np.array(act))**2\n",
    "            cost_der = np.array(temp_out)-np.array(act)\n",
    "        elif(cost_type == 'crs_ent'):\n",
    "            print(\"m is \", m)\n",
    "            if(temp_out==1):\n",
    "                temp_out-=0.00001\n",
    "            elif(temp_out==0):\n",
    "                temp_out+=0.00001\n",
    "            cost = (-1 / m) * (np.multiply(act, np.log(temp_out)) + np.multiply(1 - act, np.log(1 - temp_out)))\n",
    "            cost_der =  (-1/m)* (np.divide(act, temp_out) - np.divide(1 - act, 1 - temp_out))\n",
    "        else:\n",
    "            raise Exception('Invalid cost function given.')\n",
    "        return cost,cost_der\n",
    "\n",
    "    def _back_propagate(self,cost_der): #Backpropogate the error to calculate the gradients for all the weights and biases\\n\",\n",
    "        temp3 = np.array(cost_der)\n",
    "        temp3 = np.expand_dims(temp3,axis=1)\n",
    "        Wg = []\n",
    "        Bg = []\n",
    "        for i in reversed(range(len(self.Layers))):\n",
    "            temp1, temp2, temp3 = self.Layers[i].back_propagate(temp3)\n",
    "            Wg.insert(0,temp1)\n",
    "            Bg.insert(0,temp2),\n",
    "        return Wg,Bg\n",
    "\n",
    "    def _update_weights(self): #Updates the weights and biases with the calculated gradient\n",
    "        for i in range(len(self.Layers)):\n",
    "            self.Weights[i] = self.Weights[i] - self.learning_rate * self.WGrads[i]\n",
    "            self.Bias[i] = self.Bias[i] - self.learning_rate * self.BGrads[i]\n",
    "\n",
    "    def train(self,inps,outs,inp_val,out_val,epochs=50,learning_rate = 0.03,printCost=False,printEvery = 10, printFrom = 0):\n",
    "        #Function to run the training loop for the given number of epochs\n",
    "        self.inputs = inps\n",
    "        self.outputs = outs\n",
    "        self.inputs_val = inp_val\n",
    "        self.outputs_val = out_val\n",
    "        self.learning_rate = learning_rate\n",
    "        self._initialize()\n",
    "        cost_av = 0\n",
    "        cost_der_av = 0\n",
    "        self.n_inps = self.inputs.shape[0]\n",
    "        costs = []\n",
    "        trn_acc =[]\n",
    "        tst_acc = []\n",
    "        WGtemp = []\n",
    "        BGtemp = []\n",
    "        xaxis = []\n",
    "        for i in range(epochs):\n",
    "            for j in range(len(self.inputs)):\n",
    "                pred = self._forward_propagate(self.inputs[j])\n",
    "                print(pred,end=\"\")\n",
    "                cost,cost_der = self._compute_cost(pred,self.outputs[j])\n",
    "                WGtemp,BGtemp=self._back_propagate(cost_der)\n",
    "                cost_av+=cost\n",
    "                cost_der_av+=cost_der\n",
    "                if(j==0):\n",
    "                    self.WGrads = WGtemp\n",
    "                    self.BGrads = BGtemp\n",
    "                else:\n",
    "                    for k in range(len(self.Layers)):\n",
    "                        self.WGrads[k]=self.WGrads[k]+WGtemp[k]\n",
    "                        self.BGrads[k]=self.BGrads[k]+BGtemp[k]\n",
    "            for j in range(len(self.Layers)):\n",
    "                self.WGrads[j]=self.WGrads[j]/self.n_inps\n",
    "                self.BGrads[j]=self.BGrads[j]/self.n_inps\n",
    "            cost_av/=self.n_inps\n",
    "            cost_der_av/=self.n_inps\n",
    "            if(printCost and i%printEvery==0 and i>=printFrom):# and i!=0 and i>10):\n",
    "                print(\"Epoch %d/%d,\"%(i+1,epochs),end=\" \")\n",
    "                print(\"Cost = %f,\"%(cost_av),end=\" \")\n",
    "                _,acc1 = self.predict_classes(self.inputs,self.outputs)\n",
    "                _,acc2 = self.predict_classes(self.inputs_val, self.outputs_val)\n",
    "                print(\"Training Accuracy = %f, Validation Accuracy = %f\" %(acc1,acc2))\n",
    "                trn_acc.append(acc1)\n",
    "                tst_acc.append(acc2)\n",
    "                xaxis.append(i)\n",
    "                costs.append(cost_av[0])\n",
    "                self._showGWandBG()\n",
    "            self._update_weights()\n",
    "        print(\"Epoch %d/%d,\"%(epochs,epochs),end=\" \")\n",
    "        print(\"Cost = %f,\"%(cost_av),end=\" \")\n",
    "        acc1 = self.predict_classes(self.inputs,self.outputs)\n",
    "        acc2 = self.predict_classes(self.inputs_val, self.outputs_val)\n",
    "        print(\"Training Accuracy = %f, Validation Accuracy = %f\"%(acc1,acc2))\n",
    "        #Plotting the results\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(np.squeeze(costs)) #Plotting the cost function\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.title(\"Cost Function (Learning rate = \" + str(self.learning_rate)+\")\")\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(xaxis,trn_acc) #Plotting the training accuracies\n",
    "        plt.plot(xaxis,tst_acc) #Plotting the test accuracies\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend(['Train Acc', 'Test Acc'])\n",
    "        plt.title(\"Training and Test Acuuracies\")\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self,p): #Predicting the outputs for all the given inputs\n",
    "        preds = []\n",
    "        for temp in p:\n",
    "            for i in range(len(self.Layers)):\n",
    "                if i==0:\n",
    "                    temp = self.Layers[i].forward_propagate(temp,self.Weights[i],self.Bias[i],'relu')\n",
    "                elif i==self.n_layers:\n",
    "                    temp = self.Layers[i].forward_propagate(temp,self.Weights[i],self.Bias[i],choice='sigm')\n",
    "                else:\n",
    "                    temp = self.Layers[i].forward_propagate(temp,self.Weights[i],self.Bias[i],'relu')\n",
    "            preds.append(temp)\n",
    "        return preds\n",
    "\n",
    "    def predict_classes(self,p,q): #Predicting the classes for the given inputs and also calculating the accuracy\n",
    "        preds = np.array(self.predict(p))\n",
    "        preds[preds>0.5]=1\n",
    "        preds[preds<=0.5]=0\n",
    "        preds = np.squeeze(preds)\n",
    "        acc = np.sum((preds == q)/p.shape[0])\n",
    "        return preds,acc\n",
    "    ###END OF NEURAL_NET CLASS###"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Improting and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\o97290\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('C:/Users/o97290/Desktop/diabetes.csv')\n",
    "\n",
    "inputs = data.drop('Outcome', axis = 1)\n",
    "inputs = np.array(inputs) #Converting the input Pandas Series object into a numpy array\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "sc = MinMaxScaler() #Using a MinMaxScalar to linearly transform the data\n",
    "inputs = sc.fit_transform(inputs)\n",
    "outputs = data['Outcome'].as_matrix() #Converting the output Pandas Series object into a numpy array\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.10, random_state=42) #Splitting the test set\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 12.0) # set default size of plots\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10) #Setting a random seed for reproducability and sanity checks\n",
    "net = Neural_Net([10,5]) #Creating a Neural_Net object\n",
    "net.train(X_train,y_train,X_test,y_test,epochs=400,learning_rate=0.2,printCost=True,printEvery=1, printFrom = 0) #Training the Neural Network\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
